{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implications for Other Tasks"
      ],
      "metadata": {
        "id": "-FaSlUuYY4of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# For Google Colab environment\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Define bowling styles (same as provided)\n",
        "bowler_styles = {\n",
        "    'Nuwan Kulasekara': 'Fast', 'Lasith Malinga': 'Fast', 'Angelo Mathews': 'Medium', 'Rangana Herath': 'Spin',\n",
        "    'Tillakaratne Dilshan': 'Spin', 'Suranga Lakmal': 'Fast', 'Jeevan Mendis': 'Spin', 'Tim Southee': 'Fast',\n",
        "    'Trent Boult': 'Fast', 'Adam Milne': 'Fast', 'Daniel Vettori': 'Spin', 'Grant Elliott': 'Medium',\n",
        "    'Kane Williamson': 'Spin', 'Corey Anderson': 'Fast', 'James Anderson': 'Fast', 'Stuart Broad': 'Fast',\n",
        "    'Chris Woakes': 'Fast', 'Steven Finn': 'Fast', 'Moeen Ali': 'Spin', 'Joe Root': 'Spin', 'Mitchell Starc': 'Fast',\n",
        "    'Josh Hazlewood': 'Fast', 'Mitchell Johnson': 'Fast', 'Mitchell Marsh': 'Fast', 'Shane Watson': 'Fast',\n",
        "    'Glenn Maxwell': 'Spin', 'Steven Smith': 'Spin', 'Tinashe Panyangara': 'Fast', 'Tendai Chatara': 'Fast',\n",
        "    'Solomon Mire': 'Medium', 'Elton Chigumbura': 'Medium', 'Sean Williams': 'Spin', 'Tafadzwa Kamungozi': 'Spin',\n",
        "    'Sikandar Raza': 'Spin', 'Hamilton Masakadza': 'Medium', 'Vernon Philander': 'Fast', 'Morne Morkel': 'Fast',\n",
        "    'Dale Steyn': 'Fast', 'Farhaan Behardien': 'Medium', 'Jean-Paul Duminy': 'Spin', 'Imran Tahir': 'Spin',\n",
        "    'Mohammad Irfan': 'Fast', 'Sohail Khan': 'Fast', 'Shahid Afridi': 'Spin', 'Wahab Riaz': 'Fast',\n",
        "    'Yasir Shah': 'Spin', 'Haris Sohail': 'Spin', 'Umesh Yadav': 'Fast', 'Mohammed Shami': 'Fast',\n",
        "    'Mohit Sharma': 'Fast', 'Suresh Raina': 'Spin', 'Ravichandran Ashwin': 'Spin', 'Ravindra Jadeja': 'Spin',\n",
        "    'John Mooney': 'Medium', 'Max Sorensen': 'Fast', 'Andy McBrine': 'Spin', 'Kevin O\\'Brien': 'Medium',\n",
        "    'George Dockrell': 'Spin', 'Paul Stirling': 'Spin', 'Jason Holder': 'Fast', 'Kemar Roach': 'Fast',\n",
        "    'Jerome Taylor': 'Fast', 'Andre Russell': 'Fast', 'Chris Gayle': 'Spin', 'Darren Sammy': 'Medium',\n",
        "    'Marlon Samuels': 'Spin', 'Lendl Simmons': 'Medium', 'Iain Wardlaw': 'Fast', 'Rob Taylor': 'Medium',\n",
        "    'Josh Davey': 'Fast', 'Majid Haq': 'Spin', 'Hamid Hassan': 'Fast', 'Shapoor Zadran': 'Fast',\n",
        "    'Aftab Alam': 'Fast', 'Mirwais Ashraf': 'Medium', 'Mohammad Nabi': 'Spin', 'Javed Ahmadi': 'Spin',\n",
        "    'Samiullah Shenwari': 'Spin', 'Mashrafe Mortaza': 'Fast', 'Rubel Hossain': 'Fast', 'Taskin Ahmed': 'Fast',\n",
        "    'Shakib Al Hasan': 'Spin', 'Mahmudullah': 'Spin', 'Soumya Sarkar': 'Medium', 'Sabbir Rahman': 'Spin',\n",
        "    'Mohammad Naveed': 'Fast', 'Amjad Javed': 'Medium', 'Nasir Aziz': 'Spin', 'Mohammad Tauqir': 'Spin',\n",
        "    'Krishna Chandran': 'Medium', 'Rohan Mustafa': 'Spin', 'Sohaib Maqsood': 'Medium', 'Sulieman Benn': 'Spin',\n",
        "    'Thisara Perera': 'Fast', 'Dawlat Zadran': 'Fast', 'Asghar Stanikzai': 'Medium', 'Wayne Parnell': 'Fast',\n",
        "    'Alasdair Evans': 'Fast', 'Richie Berrington': 'Medium', 'Matt Machan': 'Spin', 'Kyle Coetzer': 'Medium',\n",
        "    'Nikita Miller': 'Spin', 'Alex Cusack': 'Medium', 'Manjula Guruge': 'Fast', 'Gulbadin Naib': 'Medium',\n",
        "    'Kyle Abbott': 'Fast', 'Faf du Plessis': 'Spin', 'Pat Cummins': 'Fast', 'Bhuvneshwar Kumar': 'Fast',\n",
        "    'Tawanda Mupariwa': 'Medium', 'Rahat Ali': 'Fast', 'Rilee Rossouw': 'Medium', 'AB de Villiers': 'Medium',\n",
        "    'Khurram Khan': 'Spin', 'Nawroz Mangal': 'Spin', 'Michael Clarke': 'Spin', 'James Faulkner': 'Fast',\n",
        "    'Nasir Hossain': 'Spin', 'Dwayne Smith': 'Medium', 'Sachithra Senanayake': 'Spin', 'Seekkuge Prasanna': 'Spin',\n",
        "    'Xavier Doherty': 'Spin', 'Chris Jordan': 'Fast', 'Arafat Sunny': 'Spin', 'Rohit Sharma': 'Spin',\n",
        "    'Stuart Thompson': 'Medium', 'Michael Leask': 'Spin', 'Dushmantha Chameera': 'Fast', 'Kamran Shazad': 'Fast',\n",
        "    'Fahad Alhashmi': 'Fast', 'Shaiman Anwar': 'Medium', 'Mitigation McClenaghan': 'Fast', 'Taijul Islam': 'Spin',\n",
        "    'Ravi Bopara': 'Medium', 'James Tredwell': 'Spin', 'Ehsan Adil': 'Fast', 'Tharindu Kaushal': 'Spin',\n",
        "    'Matt Henry': 'Fast', 'Virat Kohli': 'Medium'\n",
        "}\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive')\n",
        "    dataset_path = '/content/combined.csv'  # Adjust if needed\n",
        "    save_path_colab = '/content/drive/My Drive/lstm_embeddings/'\n",
        "    os.makedirs(save_path_colab, exist_ok=True)\n",
        "else:\n",
        "    dataset_path = 'combined.csv'  # Adjust for local path\n",
        "    save_path_local = './lstm_embeddings/'\n",
        "    os.makedirs(save_path_local, exist_ok=True)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Add bowler_type feature\n",
        "df['bowler_type'] = df['bowler_name'].map(bowler_styles)\n",
        "\n",
        "# Handle missing values\n",
        "numerical_cols = ['landing_x', 'landing_y', 'ended_x', 'ended_y', 'ball_speed', 'ovr']\n",
        "for col in numerical_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())  # Explicit assignment for pandas 3.0 compatibility\n",
        "df['bowler_type'] = df['bowler_type'].fillna('Unknown')\n",
        "\n",
        "# Define deceptive deliveries\n",
        "ewma_features = ['landing_x', 'landing_y', 'ended_x', 'ended_y', 'ball_speed']\n",
        "alpha = 0.2\n",
        "for feature in ewma_features:\n",
        "    df[f'{feature}_ewma'] = df.groupby('bowler_type')[feature].transform(\n",
        "        lambda x: x.ewm(alpha=alpha, adjust=False).mean()\n",
        "    )\n",
        "    df[f'{feature}_deviation'] = df[feature] - df[f'{feature}_ewma']\n",
        "    std_dev = df.groupby('bowler_type')[f'{feature}_deviation'].transform('std')\n",
        "    df[f'{feature}_is_deceptive'] = (df[f'{feature}_deviation'].abs() > 2 * std_dev).astype(int)\n",
        "df['is_deceptive'] = df[[f'{feat}_is_deceptive' for feat in ewma_features]].max(axis=1)\n",
        "\n",
        "# Encode bowler_type before building sequences\n",
        "le = LabelEncoder()\n",
        "df['bowler_type_encoded'] = le.fit_transform(df['bowler_type'])\n",
        "\n",
        "# Feature engineering - use bowler_type_encoded instead of bowler_type\n",
        "features = ['landing_x', 'landing_y', 'ended_x', 'ended_y', 'ball_speed', 'bowler_type_encoded', 'ovr']\n",
        "sequence_length = 5\n",
        "sequences = []\n",
        "labels = []\n",
        "sequence_ids = []\n",
        "\n",
        "# Create match_id proxy\n",
        "df['match_id'] = df.groupby(['batting_team', 'bowling_team', 'inning']).ngroup()\n",
        "df = df.sort_values(['match_id', 'ovr'])\n",
        "\n",
        "# Build sequences\n",
        "SEQ = 5\n",
        "Xs, ys = [], []\n",
        "for mid in df['match_id'].unique():\n",
        "    mdf = df[df['match_id'] == mid]\n",
        "    for bt in mdf['bowler_type'].unique():\n",
        "        bdf = mdf[mdf['bowler_type'] == bt]\n",
        "        for i in range(SEQ, len(bdf)):\n",
        "            Xs.append(bdf.iloc[i-SEQ:i][features].values)\n",
        "            ys.append(bdf.iloc[i]['is_deceptive'])\n",
        "X = np.array(Xs, dtype=np.float32)\n",
        "y = np.array(ys, dtype=np.int32)\n",
        "\n",
        "# Normalize numeric channels\n",
        "sc = StandardScaler()\n",
        "for idx in [0, 1, 2, 3, 4, 6]:  # Indices of numerical features\n",
        "    flat = X[:, :, idx].reshape(-1, 1)\n",
        "    X[:, :, idx] = sc.fit_transform(flat).reshape(X.shape[0], SEQ)\n",
        "joblib.dump(sc, 'scaler_lstm_seq5.pkl')\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build Functional LSTM\n",
        "inp = Input(shape=(SEQ, len(features)), name='inp')\n",
        "x1 = LSTM(64, name='lstm')(inp)\n",
        "x2 = Dropout(0.2, name='drop')(x1)\n",
        "x3 = Dense(32, activation='relu', name='dense')(x2)\n",
        "out = Dense(1, activation='sigmoid', name='out')(x3)\n",
        "model = Model(inputs=inp, outputs=out)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train with checkpoint\n",
        "ckpt = ModelCheckpoint('lstm_model_seq5.h5',\n",
        "                       monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20, batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[ckpt],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Load best model\n",
        "model.load_weights('lstm_model_seq5.h5')\n",
        "\n",
        "# Extract embeddings\n",
        "feat_ext = Model(inputs=model.input, outputs=model.get_layer('lstm').output)\n",
        "Xtr_feat = feat_ext.predict(X_train, verbose=0)\n",
        "Xte_feat = feat_ext.predict(X_test, verbose=0)\n",
        "\n",
        "# Save embeddings\n",
        "# Local save\n",
        "joblib.dump((Xtr_feat, y_train), 'lstm_embed_train_seq5.pkl')\n",
        "joblib.dump((Xte_feat, y_test), 'lstm_embed_test_seq5.pkl')\n",
        "\n",
        "# Save to Google Drive if in Colab\n",
        "if IN_COLAB:\n",
        "    joblib.dump((Xtr_feat, y_train), os.path.join(save_path_colab, 'lstm_embed_train_seq5.pkl'))\n",
        "    joblib.dump((Xte_feat, y_test), os.path.join(save_path_colab, 'lstm_embed_test_seq5.pkl'))\n",
        "    print(f\"Embeddings saved to Google Drive at: {save_path_colab}\")\n",
        "else:\n",
        "    joblib.dump((Xtr_feat, y_train), os.path.join(save_path_local, 'lstm_embed_train_seq5.pkl'))\n",
        "    joblib.dump((Xte_feat, y_test), os.path.join(save_path_local, 'lstm_embed_test_seq5.pkl'))\n",
        "    print(f\"Embeddings saved locally at: {save_path_local}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6OsIph8PX8p",
        "outputId": "0e64069f-aa6e-442b-9e5c-b0162fb0bd0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.84570, saving model to lstm_model_seq5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123/123 - 5s - 44ms/step - accuracy: 0.8130 - loss: 0.4850 - val_accuracy: 0.8457 - val_loss: 0.4240\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 2: val_accuracy improved from 0.84570 to 0.84672, saving model to lstm_model_seq5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123/123 - 1s - 6ms/step - accuracy: 0.8394 - loss: 0.4330 - val_accuracy: 0.8467 - val_loss: 0.4176\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 3: val_accuracy improved from 0.84672 to 0.84749, saving model to lstm_model_seq5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123/123 - 1s - 10ms/step - accuracy: 0.8413 - loss: 0.4291 - val_accuracy: 0.8475 - val_loss: 0.4204\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 4: val_accuracy improved from 0.84749 to 0.84800, saving model to lstm_model_seq5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123/123 - 1s - 10ms/step - accuracy: 0.8415 - loss: 0.4275 - val_accuracy: 0.8480 - val_loss: 0.4161\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 5: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 10ms/step - accuracy: 0.8411 - loss: 0.4251 - val_accuracy: 0.8470 - val_loss: 0.4152\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 6: val_accuracy did not improve from 0.84800\n",
            "123/123 - 2s - 12ms/step - accuracy: 0.8419 - loss: 0.4248 - val_accuracy: 0.8475 - val_loss: 0.4132\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 7: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 10ms/step - accuracy: 0.8419 - loss: 0.4236 - val_accuracy: 0.8477 - val_loss: 0.4128\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 8: val_accuracy did not improve from 0.84800\n",
            "123/123 - 2s - 18ms/step - accuracy: 0.8426 - loss: 0.4228 - val_accuracy: 0.8465 - val_loss: 0.4110\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 9: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 8ms/step - accuracy: 0.8426 - loss: 0.4216 - val_accuracy: 0.8470 - val_loss: 0.4110\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 10: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 6ms/step - accuracy: 0.8419 - loss: 0.4211 - val_accuracy: 0.8467 - val_loss: 0.4104\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 11: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 10ms/step - accuracy: 0.8418 - loss: 0.4205 - val_accuracy: 0.8475 - val_loss: 0.4102\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 12: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 10ms/step - accuracy: 0.8427 - loss: 0.4208 - val_accuracy: 0.8462 - val_loss: 0.4098\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 13: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 11ms/step - accuracy: 0.8433 - loss: 0.4190 - val_accuracy: 0.8465 - val_loss: 0.4110\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 14: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 10ms/step - accuracy: 0.8419 - loss: 0.4197 - val_accuracy: 0.8460 - val_loss: 0.4093\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 15: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 6ms/step - accuracy: 0.8422 - loss: 0.4192 - val_accuracy: 0.8470 - val_loss: 0.4088\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 16: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 10ms/step - accuracy: 0.8432 - loss: 0.4175 - val_accuracy: 0.8460 - val_loss: 0.4098\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 17: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 6ms/step - accuracy: 0.8436 - loss: 0.4166 - val_accuracy: 0.8470 - val_loss: 0.4120\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 18: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 6ms/step - accuracy: 0.8433 - loss: 0.4167 - val_accuracy: 0.8472 - val_loss: 0.4093\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 19: val_accuracy did not improve from 0.84800\n",
            "123/123 - 2s - 12ms/step - accuracy: 0.8436 - loss: 0.4165 - val_accuracy: 0.8470 - val_loss: 0.4097\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 20: val_accuracy did not improve from 0.84800\n",
            "123/123 - 1s - 9ms/step - accuracy: 0.8433 - loss: 0.4164 - val_accuracy: 0.8472 - val_loss: 0.4089\n",
            "Embeddings saved to Google Drive at: /content/drive/My Drive/lstm_embeddings/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset and preprocess\n",
        "df = pd.read_csv('combined.csv')\n",
        "\n",
        "# Define bowling styles (same as provided)\n",
        "bowler_styles = {\n",
        "    'Nuwan Kulasekara': 'Fast', 'Lasith Malinga': 'Fast', 'Angelo Mathews': 'Medium', 'Rangana Herath': 'Spin',\n",
        "    'Tillakaratne Dilshan': 'Spin', 'Suranga Lakmal': 'Fast', 'Jeevan Mendis': 'Spin', 'Tim Southee': 'Fast',\n",
        "    'Trent Boult': 'Fast', 'Adam Milne': 'Fast', 'Daniel Vettori': 'Spin', 'Grant Elliott': 'Medium',\n",
        "    'Kane Williamson': 'Spin', 'Corey Anderson': 'Fast', 'James Anderson': 'Fast', 'Stuart Broad': 'Fast',\n",
        "    'Chris Woakes': 'Fast', 'Steven Finn': 'Fast', 'Moeen Ali': 'Spin', 'Joe Root': 'Spin', 'Mitchell Starc': 'Fast',\n",
        "    'Josh Hazlewood': 'Fast', 'Mitchell Johnson': 'Fast', 'Mitchell Marsh': 'Fast', 'Shane Watson': 'Fast',\n",
        "    'Glenn Maxwell': 'Spin', 'Steven Smith': 'Spin', 'Tinashe Panyangara': 'Fast', 'Tendai Chatara': 'Fast',\n",
        "    'Solomon Mire': 'Medium', 'Elton Chigumbura': 'Medium', 'Sean Williams': 'Spin', 'Tafadzwa Kamungozi': 'Spin',\n",
        "    'Sikandar Raza': 'Spin', 'Hamilton Masakadza': 'Medium', 'Vernon Philander': 'Fast', 'Morne Morkel': 'Fast',\n",
        "    'Dale Steyn': 'Fast', 'Farhaan Behardien': 'Medium', 'Jean-Paul Duminy': 'Spin', 'Imran Tahir': 'Spin',\n",
        "    'Mohammad Irfan': 'Fast', 'Sohail Khan': 'Fast', 'Shahid Afridi': 'Spin', 'Wahab Riaz': 'Fast',\n",
        "    'Yasir Shah': 'Spin', 'Haris Sohail': 'Spin', 'Umesh Yadav': 'Fast', 'Mohammed Shami': 'Fast',\n",
        "    'Mohit Sharma': 'Fast', 'Suresh Raina': 'Spin', 'Ravichandran Ashwin': 'Spin', 'Ravindra Jadeja': 'Spin',\n",
        "    'John Mooney': 'Medium', 'Max Sorensen': 'Fast', 'Andy McBrine': 'Spin', 'Kevin O\\'Brien': 'Medium',\n",
        "    'George Dockrell': 'Spin', 'Paul Stirling': 'Spin', 'Jason Holder': 'Fast', 'Kemar Roach': 'Fast',\n",
        "    'Jerome Taylor': 'Fast', 'Andre Russell': 'Fast', 'Chris Gayle': 'Spin', 'Darren Sammy': 'Medium',\n",
        "    'Marlon Samuels': 'Spin', 'Lendl Simmons': 'Medium', 'Iain Wardlaw': 'Fast', 'Rob Taylor': 'Medium',\n",
        "    'Josh Davey': 'Fast', 'Majid Haq': 'Spin', 'Hamid Hassan': 'Fast', 'Shapoor Zadran': 'Fast',\n",
        "    'Aftab Alam': 'Fast', 'Mirwais Ashraf': 'Medium', 'Mohammad Nabi': 'Spin', 'Javed Ahmadi': 'Spin',\n",
        "    'Samiullah Shenwari': 'Spin', 'Mashrafe Mortaza': 'Fast', 'Rubel Hossain': 'Fast', 'Taskin Ahmed': 'Fast',\n",
        "    'Shakib Al Hasan': 'Spin', 'Mahmudullah': 'Spin', 'Soumya Sarkar': 'Medium', 'Sabbir Rahman': 'Spin',\n",
        "    'Mohammad Naveed': 'Fast', 'Amjad Javed': 'Medium', 'Nasir Aziz': 'Spin', 'Mohammad Tauqir': 'Spin',\n",
        "    'Krishna Chandran': 'Medium', 'Rohan Mustafa': 'Spin', 'Sohaib Maqsood': 'Medium', 'Sulieman Benn': 'Spin',\n",
        "    'Thisara Perera': 'Fast', 'Dawlat Zadran': 'Fast', 'Asghar Stanikzai': 'Medium', 'Wayne Parnell': 'Fast',\n",
        "    'Alasdair Evans': 'Fast', 'Richie Berrington': 'Medium', 'Matt Machan': 'Spin', 'Kyle Coetzer': 'Medium',\n",
        "    'Nikita Miller': 'Spin', 'Alex Cusack': 'Medium', 'Manjula Guruge': 'Fast', 'Gulbadin Naib': 'Medium',\n",
        "    'Kyle Abbott': 'Fast', 'Faf du Plessis': 'Spin', 'Pat Cummins': 'Fast', 'Bhuvneshwar Kumar': 'Fast',\n",
        "    'Tawanda Mupariwa': 'Medium', 'Rahat Ali': 'Fast', 'Rilee Rossouw': 'Medium', 'AB de Villiers': 'Medium',\n",
        "    'Khurram Khan': 'Spin', 'Nawroz Mangal': 'Spin', 'Michael Clarke': 'Spin', 'James Faulkner': 'Fast',\n",
        "    'Nasir Hossain': 'Spin', 'Dwayne Smith': 'Medium', 'Sachithra Senanayake': 'Spin', 'Seekkuge Prasanna': 'Spin',\n",
        "    'Xavier Doherty': 'Spin', 'Chris Jordan': 'Fast', 'Arafat Sunny': 'Spin', 'Rohit Sharma': 'Spin',\n",
        "    'Stuart Thompson': 'Medium', 'Michael Leask': 'Spin', 'Dushmantha Chameera': 'Fast', 'Kamran Shazad': 'Fast',\n",
        "    'Fahad Alhashmi': 'Fast', 'Shaiman Anwar': 'Medium', 'Mitigation McClenaghan': 'Fast', 'Taijul Islam': 'Spin',\n",
        "    'Ravi Bopara': 'Medium', 'James Tredwell': 'Spin', 'Ehsan Adil': 'Fast', 'Tharindu Kaushal': 'Spin',\n",
        "    'Matt Henry': 'Fast', 'Virat Kohli': 'Medium'\n",
        "}\n",
        "\n",
        "# Add bowler_type feature\n",
        "df['bowler_type'] = df['bowler_name'].map(bowler_styles)\n",
        "\n",
        "# Handle missing values\n",
        "numerical_cols = ['landing_x', 'landing_y', 'ended_x', 'ended_y', 'ball_speed', 'ovr']\n",
        "for col in numerical_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())  # Explicit assignment for pandas 3.0 compatibility\n",
        "df['bowler_type'] = df['bowler_type'].fillna('Unknown')\n",
        "\n",
        "# Define deceptive deliveries\n",
        "ewma_features = ['landing_x', 'landing_y', 'ended_x', 'ended_y', 'ball_speed']\n",
        "alpha = 0.2\n",
        "for feature in ewma_features:\n",
        "    df[f'{feature}_ewma'] = df.groupby('bowler_type')[feature].transform(\n",
        "        lambda x: x.ewm(alpha=alpha, adjust=False).mean()\n",
        "    )\n",
        "    df[f'{feature}_deviation'] = df[feature] - df[f'{feature}_ewma']\n",
        "    std_dev = df.groupby('bowler_type')[f'{feature}_deviation'].transform('std')\n",
        "    df[f'{feature}_is_deceptive'] = (df[f'{feature}_deviation'].abs() > 2 * std_dev).astype(int)\n",
        "df['is_deceptive'] = df[[f'{feat}_is_deceptive' for feat in ewma_features]].max(axis=1)\n",
        "\n",
        "# Create match_id proxy\n",
        "df['match_id'] = df.groupby(['batting_team', 'bowling_team', 'inning']).ngroup()\n",
        "df = df.sort_values(['match_id', 'ovr'])\n",
        "\n",
        "# Encode bowler_type before defining features\n",
        "le = LabelEncoder()\n",
        "df['bowler_type_encoded'] = le.fit_transform(df['bowler_type'])\n",
        "\n",
        "# Feature engineering - use bowler_type_encoded instead of bowler_type\n",
        "features = ['landing_x', 'landing_y', 'ended_x', 'ended_y', 'ball_speed', 'bowler_type_encoded', 'ovr']\n",
        "sequence_length = 5\n",
        "sequences = []\n",
        "labels = []\n",
        "sequence_ids = []\n",
        "\n",
        "# Build sequences for seq_len=5\n",
        "SEQ = 5\n",
        "Xs, ys = [], []\n",
        "for mid in df['match_id'].unique():\n",
        "    mdf = df[df['match_id'] == mid]\n",
        "    for bt in mdf['bowler_type'].unique():\n",
        "        bdf = mdf[mdf['bowler_type'] == bt]\n",
        "        for i in range(SEQ, len(bdf)):\n",
        "            Xs.append(bdf.iloc[i-SEQ:i][features].values)\n",
        "            ys.append(bdf.iloc[i]['is_deceptive'])\n",
        "X = np.array(Xs, dtype=np.float32)\n",
        "y = np.array(ys, dtype=np.int32)\n",
        "\n",
        "# Normalize numeric channels\n",
        "sc = StandardScaler()\n",
        "for idx in [0, 1, 2, 3, 4, 6]:  # Indices for numerical features (excluding bowler_type_encoded at index 5)\n",
        "    flat = X[:, :, idx].reshape(-1, 1)\n",
        "    X[:, :, idx] = sc.fit_transform(flat).reshape(X.shape[0], SEQ)\n",
        "joblib.dump(sc, f'scaler_lstm_seq{SEQ}.pkl')\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load existing embeddings and ensure y_train, y_test are numerical\n",
        "Xtr_feat, y_train_loaded = joblib.load('lstm_embed_train_seq5.pkl')\n",
        "Xte_feat, y_test_loaded = joblib.load('lstm_embed_test_seq5.pkl')\n",
        "\n",
        "# Explicitly cast y_train and y_test to float32\n",
        "y_train = np.array(y_train_loaded, dtype=np.float32)\n",
        "y_test = np.array(y_test_loaded, dtype=np.float32)\n",
        "\n",
        "# Debug: Print data types to confirm\n",
        "print(\"y_train dtype:\", y_train.dtype)\n",
        "print(\"y_test dtype:\", y_test.dtype)\n",
        "print(\"X_train dtype:\", X_train.dtype)\n",
        "print(\"X_test dtype:\", X_test.dtype)\n",
        "\n",
        "# 1. Zero-Shot Learning (ZSL)\n",
        "# Simulate unseen bowler types by excluding some types during training\n",
        "seen_bowler_types = ['Fast', 'Spin']  # Train on these\n",
        "unseen_bowler_types = ['Medium']  # Test on these\n",
        "\n",
        "# Split data based on bowler types\n",
        "train_indices = df[df['bowler_type'].isin(seen_bowler_types)].index\n",
        "test_indices = df[df['bowler_type'].isin(unseen_bowler_types)].index\n",
        "\n",
        "# Rebuild sequences for ZSL\n",
        "Xs_seen, ys_seen = [], []\n",
        "Xs_unseen, ys_unseen = [], []\n",
        "for mid in df['match_id'].unique():\n",
        "    mdf = df[df['match_id'] == mid]\n",
        "    for bt in mdf['bowler_type'].unique():\n",
        "        bdf = mdf[mdf['bowler_type'] == bt]\n",
        "        for i in range(SEQ, len(bdf)):\n",
        "            seq_data = bdf.iloc[i-SEQ:i][features].values\n",
        "            label = bdf.iloc[i]['is_deceptive']\n",
        "            if bdf.iloc[i].name in train_indices:\n",
        "                Xs_seen.append(seq_data)\n",
        "                ys_seen.append(label)\n",
        "            elif bdf.iloc[i].name in test_indices:\n",
        "                Xs_unseen.append(seq_data)\n",
        "                ys_unseen.append(label)\n",
        "\n",
        "X_seen = np.array(Xs_seen, dtype=np.float32)\n",
        "y_seen = np.array(ys_seen, dtype=np.int32)\n",
        "X_unseen = np.array(Xs_unseen, dtype=np.float32)\n",
        "y_unseen = np.array(ys_unseen, dtype=np.int32)\n",
        "\n",
        "# Normalize for ZSL\n",
        "for idx in [0, 1, 2, 3, 4, 6]:\n",
        "    flat_seen = X_seen[:, :, idx].reshape(-1, 1)\n",
        "    X_seen[:, :, idx] = sc.transform(flat_seen).reshape(X_seen.shape[0], SEQ)\n",
        "    flat_unseen = X_unseen[:, :, idx].reshape(-1, 1)\n",
        "    X_unseen[:, :, idx] = sc.transform(flat_unseen).reshape(X_unseen.shape[0], SEQ)\n",
        "\n",
        "# Extract embeddings for ZSL\n",
        "inp = Input(shape=(SEQ, len(features)), name='input_seq')\n",
        "x1 = LSTM(64, name='lstm_layer')(inp)\n",
        "x2 = Dropout(0.2, name='dropout')(x1)\n",
        "feat_ext = Model(inputs=inp, outputs=x1)\n",
        "X_seen_embed = feat_ext.predict(X_seen, verbose=0)\n",
        "X_unseen_embed = feat_ext.predict(X_unseen, verbose=0)\n",
        "\n",
        "# Simple ZSL: Use mean embeddings of seen classes as prototypes\n",
        "prototypes = {}\n",
        "for bt in seen_bowler_types:\n",
        "    indices = df[df['bowler_type'] == bt].index\n",
        "    embed_indices = [i for i, idx in enumerate(df.index) if idx in indices and i < len(X_seen_embed)]\n",
        "    if embed_indices:\n",
        "        deceptive_embed = X_seen_embed[embed_indices][y_seen[embed_indices] == 1].mean(axis=0)\n",
        "        non_deceptive_embed = X_seen_embed[embed_indices][y_seen[embed_indices] == 0].mean(axis=0)\n",
        "        prototypes[bt] = {'deceptive': deceptive_embed, 'non_deceptive': non_deceptive_embed}\n",
        "\n",
        "# Predict for unseen bowler types using cosine similarity\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "y_pred_zsl = []\n",
        "for embed in X_unseen_embed:\n",
        "    max_sim_deceptive = -1\n",
        "    max_sim_non_deceptive = -1\n",
        "    for bt in unseen_bowler_types:\n",
        "        if bt in prototypes:\n",
        "            sim_deceptive = cosine_similarity(embed, prototypes[bt]['deceptive'])\n",
        "            sim_non_deceptive = cosine_similarity(embed, prototypes[bt]['non_deceptive'])\n",
        "            max_sim_deceptive = max(max_sim_deceptive, sim_deceptive)\n",
        "            max_sim_non_deceptive = max(max_sim_non_deceptive, sim_non_deceptive)\n",
        "    y_pred_zsl.append(1 if max_sim_deceptive > max_sim_non_deceptive else 0)\n",
        "\n",
        "y_pred_zsl = np.array(y_pred_zsl)\n",
        "print(\"ZSL Accuracy for Unseen Bowler Types (Medium):\", accuracy_score(y_unseen, y_pred_zsl))\n",
        "\n",
        "# 2. Linear Probing with Contrastive Loss\n",
        "# Custom contrastive loss function\n",
        "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
        "\n",
        "# Build model with contrastive loss\n",
        "inp1 = Input(shape=(SEQ, len(features)), name='input1')\n",
        "inp2 = Input(shape=(SEQ, len(features)), name='input2')\n",
        "label_input = Input(shape=(1,), name='pair_label')  # 1 if same class, 0 if different\n",
        "\n",
        "# Shared LSTM\n",
        "lstm = LSTM(64, name='shared_lstm')\n",
        "x1 = lstm(inp1)\n",
        "x2 = lstm(inp2)\n",
        "x1 = Dropout(0.2, name='dropout1')(x1)\n",
        "x2 = Dropout(0.2, name='dropout2')(x2)\n",
        "\n",
        "# Compute Euclidean distance between embeddings\n",
        "distance = Lambda(lambda tensors: K.sqrt(K.sum(K.square(tensors[0] - tensors[1]), axis=1, keepdims=True)),\n",
        "                  name='distance')([x1, x2])\n",
        "\n",
        "# Classification branch\n",
        "x3 = Dense(32, activation='relu', name='dense_layer')(x1)\n",
        "class_output = Dense(1, activation='sigmoid', name='classification')(x3)\n",
        "\n",
        "# Model with two inputs and two outputs\n",
        "model_contrastive = Model(inputs=[inp1, inp2, label_input], outputs=[class_output, distance])\n",
        "model_contrastive.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                          loss=['binary_crossentropy', contrastive_loss],\n",
        "                          loss_weights=[1.0, 0.5])\n",
        "\n",
        "# Generate pairs for contrastive loss\n",
        "def generate_pairs(X, y):\n",
        "    indices = np.random.permutation(len(X))\n",
        "    X1, X2, pair_labels, y_labels = [], [], [], []\n",
        "    for i in indices:\n",
        "        pos_indices = np.where(y == y[i])[0]\n",
        "        pos_idx = np.random.choice(pos_indices)\n",
        "        neg_indices = np.where(y != y[i])[0]\n",
        "        neg_idx = np.random.choice(neg_indices)\n",
        "        X1.extend([X[i], X[i]])\n",
        "        X2.extend([X[pos_idx], X[neg_idx]])\n",
        "        pair_labels.extend([1, 0])\n",
        "        y_labels.extend([y[i], y[i]])\n",
        "    X1 = np.array(X1, dtype=np.float32)\n",
        "    X2 = np.array(X2, dtype=np.float32)\n",
        "    pair_labels = np.array(pair_labels, dtype=np.float32)\n",
        "    y_labels = np.array(y_labels, dtype=np.float32)\n",
        "    print(\"X1 shape:\", X1.shape, \"dtype:\", X1.dtype)\n",
        "    print(\"X2 shape:\", X2.shape, \"dtype:\", X2.dtype)\n",
        "    print(\"pair_labels shape:\", pair_labels.shape, \"dtype:\", pair_labels.dtype)\n",
        "    print(\"y_labels shape:\", y_labels.shape, \"dtype:\", y_labels.dtype)\n",
        "    return X1, X2, pair_labels, y_labels\n",
        "\n",
        "# Create TensorFlow datasets for training and validation with flattened structure\n",
        "def create_dataset(X, y, batch_size):\n",
        "    X1, X2, pair_labels, y_labels = generate_pairs(X, y)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            (X1, X2, pair_labels),  # Inputs as a tuple\n",
        "            (y_labels, pair_labels)  # Outputs as a tuple\n",
        "        )\n",
        "    )\n",
        "    dataset = dataset.map(\n",
        "        lambda inputs, outputs: (\n",
        "            (\n",
        "                tf.cast(inputs[0], tf.float32),  # X1\n",
        "                tf.cast(inputs[1], tf.float32),  # X2\n",
        "                tf.cast(inputs[2], tf.float32)   # pair_labels\n",
        "            ),\n",
        "            (\n",
        "                tf.cast(outputs[0], tf.float32),  # y_labels\n",
        "                tf.cast(outputs[1], tf.float32)   # pair_labels\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    for batch in dataset.take(1):\n",
        "        print(\"Dataset inputs dtypes:\", [x.dtype for x in batch[0]])\n",
        "        print(\"Dataset outputs dtypes:\", [x.dtype for x in batch[1]])\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Create datasets\n",
        "batch_size = 128\n",
        "train_dataset = create_dataset(X_train, y_train, batch_size)\n",
        "val_dataset = create_dataset(X_test, y_test, batch_size)\n",
        "\n",
        "# Calculate steps\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "val_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train with contrastive loss using the dataset\n",
        "history = model_contrastive.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_steps,\n",
        "    epochs=20,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Extract improved embeddings using the trained LSTM\n",
        "feat_ext_contrastive = Model(inputs=inp1, outputs=x1)\n",
        "Xtr_feat_contrastive = feat_ext_contrastive.predict(X_train, verbose=0)\n",
        "Xte_feat_contrastive = feat_ext_contrastive.predict(X_test, verbose=0)\n",
        "\n",
        "# Linear probing on improved embeddings\n",
        "probe_contrastive = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "probe_contrastive.fit(Xtr_feat_contrastive, y_train)\n",
        "print(\"Improved Linear Probing Acc (64D):\", probe_contrastive.score(Xte_feat_contrastive, y_test))\n",
        "\n",
        "# 3. Transfer Learning: Predict Boundary Deliveries\n",
        "# Simulate a new task: predict if a delivery results in a boundary (4 or 6)\n",
        "if 'is_boundary' not in df.columns:\n",
        "    df['is_boundary'] = ((df['ended_x'].abs() > 50) | (df['ended_y'].abs() > 50)).astype(int)\n",
        "\n",
        "# Build sequences for the new task\n",
        "Xs_boundary, ys_boundary = [], []\n",
        "for mid in df['match_id'].unique():\n",
        "    mdf = df[df['match_id'] == mid]\n",
        "    for bt in mdf['bowler_type'].unique():\n",
        "        bdf = mdf[mdf['bowler_type'] == bt]\n",
        "        for i in range(SEQ, len(bdf)):\n",
        "            Xs_boundary.append(bdf.iloc[i-SEQ:i][features].values)\n",
        "            ys_boundary.append(bdf.iloc[i]['is_boundary'])\n",
        "X_boundary = np.array(Xs_boundary, dtype=np.float32)\n",
        "y_boundary = np.array(ys_boundary, dtype=np.int32)\n",
        "\n",
        "# Normalize\n",
        "for idx in [0, 1, 2, 3, 4, 6]:\n",
        "    flat_boundary = X_boundary[:, :, idx].reshape(-1, 1)\n",
        "    X_boundary[:, :, idx] = sc.transform(flat_boundary).reshape(X_boundary.shape[0], SEQ)\n",
        "\n",
        "# Train/test split for the new task\n",
        "X_train_boundary, X_test_boundary, y_train_boundary, y_test_boundary = train_test_split(\n",
        "    X_boundary, y_boundary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a fresh model for transfer learning instead of reusing base_model\n",
        "boundary_input = Input(shape=(SEQ, len(features)), name='boundary_input')\n",
        "boundary_lstm = LSTM(64, name='boundary_lstm')(boundary_input)\n",
        "boundary_dropout = Dropout(0.2, name='boundary_dropout')(boundary_lstm)\n",
        "boundary_dense = Dense(32, activation='relu', name='boundary_dense')(boundary_dropout)\n",
        "boundary_dropout2 = Dropout(0.2, name='boundary_dropout2')(boundary_dense)\n",
        "boundary_out = Dense(1, activation='sigmoid', name='boundary_output')(boundary_dropout2)\n",
        "\n",
        "# Build and compile the boundary model\n",
        "boundary_model = Model(inputs=boundary_input, outputs=boundary_out)\n",
        "boundary_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train on the new task\n",
        "boundary_model.fit(X_train_boundary, y_train_boundary,\n",
        "                   epochs=10, batch_size=128,\n",
        "                   validation_split=0.2, verbose=2)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_boundary = (boundary_model.predict(X_test_boundary) > 0.5).astype(int).flatten()\n",
        "print(\"Transfer Learning Acc (Boundary Prediction):\", accuracy_score(y_test_boundary, y_pred_boundary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_3EkKgjXZZH",
        "outputId": "511b588e-c451-42e8-b4de-36816c4d1ca5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train dtype: float32\n",
            "y_test dtype: float32\n",
            "X_train dtype: float32\n",
            "X_test dtype: float32\n",
            "ZSL Accuracy for Unseen Bowler Types (Medium): 0.8192939531632296\n",
            "X1 shape: (39206, 5, 7) dtype: float32\n",
            "X2 shape: (39206, 5, 7) dtype: float32\n",
            "pair_labels shape: (39206,) dtype: float32\n",
            "y_labels shape: (39206,) dtype: float32\n",
            "Dataset inputs dtypes: [tf.float32, tf.float32, tf.float32]\n",
            "Dataset outputs dtypes: [tf.float32, tf.float32]\n",
            "X1 shape: (9802, 5, 7) dtype: float32\n",
            "X2 shape: (9802, 5, 7) dtype: float32\n",
            "pair_labels shape: (9802,) dtype: float32\n",
            "y_labels shape: (9802,) dtype: float32\n",
            "Dataset inputs dtypes: [tf.float32, tf.float32, tf.float32]\n",
            "Dataset outputs dtypes: [tf.float32, tf.float32]\n",
            "Epoch 1/20\n",
            "153/153 - 4s - 25ms/step - classification_loss: 0.4775 - distance_loss: 0.3746 - loss: 0.6648 - val_classification_loss: 0.4281 - val_distance_loss: 0.2799 - val_loss: 0.5681\n",
            "Epoch 2/20\n",
            "153/153 - 1s - 8ms/step - classification_loss: 0.4306 - distance_loss: 0.2552 - loss: 0.5582 - val_classification_loss: 0.4255 - val_distance_loss: 0.2829 - val_loss: 0.5670\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153/153 - 0s - 2ms/step - classification_loss: 0.5372 - distance_loss: 0.2471 - loss: 0.6608 - val_classification_loss: 0.4255 - val_distance_loss: 0.2831 - val_loss: 0.5670\n",
            "Epoch 4/20\n",
            "153/153 - 1s - 8ms/step - classification_loss: 0.4261 - distance_loss: 0.2514 - loss: 0.5518 - val_classification_loss: 0.4262 - val_distance_loss: 0.2882 - val_loss: 0.5703\n",
            "Epoch 5/20\n",
            "153/153 - 1s - 7ms/step - classification_loss: 0.4286 - distance_loss: 0.2523 - loss: 0.5548 - val_classification_loss: 0.4254 - val_distance_loss: 0.2859 - val_loss: 0.5683\n",
            "Epoch 6/20\n",
            "153/153 - 0s - 2ms/step - classification_loss: 0.5565 - distance_loss: 0.2520 - loss: 0.6825 - val_classification_loss: 0.4253 - val_distance_loss: 0.2860 - val_loss: 0.5684\n",
            "Epoch 7/20\n",
            "153/153 - 5s - 34ms/step - classification_loss: 0.4241 - distance_loss: 0.2504 - loss: 0.5493 - val_classification_loss: 0.4256 - val_distance_loss: 0.2900 - val_loss: 0.5706\n",
            "Epoch 8/20\n",
            "153/153 - 2s - 16ms/step - classification_loss: 0.4266 - distance_loss: 0.2514 - loss: 0.5523 - val_classification_loss: 0.4247 - val_distance_loss: 0.2854 - val_loss: 0.5674\n",
            "Epoch 9/20\n",
            "153/153 - 0s - 930us/step - classification_loss: 0.5509 - distance_loss: 0.2486 - loss: 0.6752 - val_classification_loss: 0.4247 - val_distance_loss: 0.2855 - val_loss: 0.5674\n",
            "Epoch 10/20\n",
            "153/153 - 1s - 8ms/step - classification_loss: 0.4221 - distance_loss: 0.2493 - loss: 0.5467 - val_classification_loss: 0.4252 - val_distance_loss: 0.2908 - val_loss: 0.5706\n",
            "Epoch 11/20\n",
            "153/153 - 1s - 8ms/step - classification_loss: 0.4246 - distance_loss: 0.2507 - loss: 0.5500 - val_classification_loss: 0.4248 - val_distance_loss: 0.2849 - val_loss: 0.5673\n",
            "Epoch 12/20\n",
            "153/153 - 0s - 1ms/step - classification_loss: 0.5703 - distance_loss: 0.2496 - loss: 0.6950 - val_classification_loss: 0.4246 - val_distance_loss: 0.2853 - val_loss: 0.5673\n",
            "Epoch 13/20\n",
            "153/153 - 5s - 33ms/step - classification_loss: 0.4215 - distance_loss: 0.2488 - loss: 0.5459 - val_classification_loss: 0.4249 - val_distance_loss: 0.2889 - val_loss: 0.5693\n",
            "Epoch 14/20\n",
            "153/153 - 2s - 11ms/step - classification_loss: 0.4243 - distance_loss: 0.2500 - loss: 0.5493 - val_classification_loss: 0.4242 - val_distance_loss: 0.2842 - val_loss: 0.5663\n",
            "Epoch 15/20\n",
            "153/153 - 0s - 2ms/step - classification_loss: 0.5546 - distance_loss: 0.2603 - loss: 0.6847 - val_classification_loss: 0.4241 - val_distance_loss: 0.2844 - val_loss: 0.5663\n",
            "Epoch 16/20\n",
            "153/153 - 2s - 16ms/step - classification_loss: 0.4201 - distance_loss: 0.2487 - loss: 0.5445 - val_classification_loss: 0.4241 - val_distance_loss: 0.2869 - val_loss: 0.5675\n",
            "Epoch 17/20\n",
            "153/153 - 1s - 7ms/step - classification_loss: 0.4231 - distance_loss: 0.2496 - loss: 0.5479 - val_classification_loss: 0.4242 - val_distance_loss: 0.2823 - val_loss: 0.5654\n",
            "Epoch 18/20\n",
            "153/153 - 0s - 2ms/step - classification_loss: 0.5437 - distance_loss: 0.2424 - loss: 0.6650 - val_classification_loss: 0.4241 - val_distance_loss: 0.2826 - val_loss: 0.5654\n",
            "Epoch 19/20\n",
            "153/153 - 5s - 33ms/step - classification_loss: 0.4186 - distance_loss: 0.2486 - loss: 0.5429 - val_classification_loss: 0.4232 - val_distance_loss: 0.2860 - val_loss: 0.5662\n",
            "Epoch 20/20\n",
            "153/153 - 1s - 8ms/step - classification_loss: 0.4227 - distance_loss: 0.2490 - loss: 0.5472 - val_classification_loss: 0.4235 - val_distance_loss: 0.2825 - val_loss: 0.5647\n",
            "Improved Linear Probing Acc (64D): 0.6563966537441338\n",
            "Epoch 1/10\n",
            "123/123 - 3s - 21ms/step - accuracy: 0.9940 - loss: 0.0533 - val_accuracy: 1.0000 - val_loss: 1.1482e-04\n",
            "Epoch 2/10\n",
            "123/123 - 1s - 7ms/step - accuracy: 0.9999 - loss: 7.8140e-04 - val_accuracy: 1.0000 - val_loss: 4.7027e-05\n",
            "Epoch 3/10\n",
            "123/123 - 1s - 12ms/step - accuracy: 0.9999 - loss: 8.9370e-04 - val_accuracy: 1.0000 - val_loss: 2.4155e-05\n",
            "Epoch 4/10\n",
            "123/123 - 1s - 10ms/step - accuracy: 0.9999 - loss: 6.5780e-04 - val_accuracy: 1.0000 - val_loss: 1.9321e-05\n",
            "Epoch 5/10\n",
            "123/123 - 1s - 6ms/step - accuracy: 0.9999 - loss: 7.9441e-04 - val_accuracy: 1.0000 - val_loss: 1.5642e-05\n",
            "Epoch 6/10\n",
            "123/123 - 1s - 6ms/step - accuracy: 0.9999 - loss: 7.0939e-04 - val_accuracy: 1.0000 - val_loss: 1.7450e-05\n",
            "Epoch 7/10\n",
            "123/123 - 1s - 6ms/step - accuracy: 0.9999 - loss: 7.4825e-04 - val_accuracy: 1.0000 - val_loss: 1.2793e-05\n",
            "Epoch 8/10\n",
            "123/123 - 1s - 10ms/step - accuracy: 0.9999 - loss: 8.3985e-04 - val_accuracy: 1.0000 - val_loss: 1.3981e-05\n",
            "Epoch 9/10\n",
            "123/123 - 1s - 6ms/step - accuracy: 0.9999 - loss: 8.3645e-04 - val_accuracy: 1.0000 - val_loss: 9.4720e-06\n",
            "Epoch 10/10\n",
            "123/123 - 1s - 7ms/step - accuracy: 0.9999 - loss: 7.6827e-04 - val_accuracy: 1.0000 - val_loss: 8.1845e-06\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Transfer Learning Acc (Boundary Prediction): 1.0\n"
          ]
        }
      ]
    }
  ]
}